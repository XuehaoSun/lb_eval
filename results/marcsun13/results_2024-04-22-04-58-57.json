{
    "config_general": {
        "lighteval_sha": "1.4",
        "num_few_shot_default": null,
        "num_fewshot_seeds": null,
        "override_batch_size": null,
        "max_samples": null,
        "job_id": -1,
        "start_time": null,
        "end_time": "2024-04-22-04-58-57",
        "total_evaluation_time_secondes": "",
        "model_name": "marcsun13/opt-350m-gptq-4bit",
        "model_sha": "",
        "model_dtype": "4bit",
        "model_size": 0.35
    },
    "results": {
        "harness|hendrycksTest-virology|0": {
            "acc": 0.3192771084337349,
            "acc_stderr": 0.0362933532994786,
            "acc_norm": 0.3192771084337349,
            "acc_norm_stderr": 0.0362933532994786
        },
        "harness|hendrycksTest-miscellaneous|0": {
            "acc": 0.2681992337164751,
            "acc_stderr": 0.01584243083526943,
            "acc_norm": 0.2681992337164751,
            "acc_norm_stderr": 0.01584243083526943
        },
        "harness|hendrycksTest-human_sexuality|0": {
            "acc": 0.2595419847328244,
            "acc_stderr": 0.03844876139785271,
            "acc_norm": 0.2595419847328244,
            "acc_norm_stderr": 0.03844876139785271
        },
        "harness|hendrycksTest-econometrics|0": {
            "acc": 0.2719298245614035,
            "acc_stderr": 0.04185774424022057,
            "acc_norm": 0.2719298245614035,
            "acc_norm_stderr": 0.04185774424022057
        },
        "harness|hendrycksTest-college_physics|0": {
            "acc": 0.30392156862745096,
            "acc_stderr": 0.045766654032077615,
            "acc_norm": 0.30392156862745096,
            "acc_norm_stderr": 0.045766654032077615
        },
        "harness|hendrycksTest-high_school_physics|0": {
            "acc": 0.2119205298013245,
            "acc_stderr": 0.033367670865679766,
            "acc_norm": 0.2119205298013245,
            "acc_norm_stderr": 0.033367670865679766
        },
        "harness|hendrycksTest-college_chemistry|0": {
            "acc": 0.17,
            "acc_stderr": 0.0377525168068637,
            "acc_norm": 0.17,
            "acc_norm_stderr": 0.0377525168068637
        },
        "harness|hendrycksTest-formal_logic|0": {
            "acc": 0.24603174603174602,
            "acc_stderr": 0.03852273364924318,
            "acc_norm": 0.24603174603174602,
            "acc_norm_stderr": 0.03852273364924318
        },
        "harness|hendrycksTest-moral_scenarios|0": {
            "acc": 0.22905027932960895,
            "acc_stderr": 0.014054314935614544,
            "acc_norm": 0.22905027932960895,
            "acc_norm_stderr": 0.014054314935614544
        },
        "harness|hendrycksTest-professional_law|0": {
            "acc": 0.24902216427640156,
            "acc_stderr": 0.01104489226404077,
            "acc_norm": 0.24902216427640156,
            "acc_norm_stderr": 0.01104489226404077
        },
        "harness|hendrycksTest-human_aging|0": {
            "acc": 0.36771300448430494,
            "acc_stderr": 0.03236198350928275,
            "acc_norm": 0.36771300448430494,
            "acc_norm_stderr": 0.03236198350928275
        },
        "harness|hendrycksTest-computer_security|0": {
            "acc": 0.32,
            "acc_stderr": 0.04688261722621504,
            "acc_norm": 0.32,
            "acc_norm_stderr": 0.04688261722621504
        },
        "harness|hendrycksTest-high_school_statistics|0": {
            "acc": 0.19444444444444445,
            "acc_stderr": 0.026991454502036726,
            "acc_norm": 0.19444444444444445,
            "acc_norm_stderr": 0.026991454502036726
        },
        "harness|hendrycksTest-college_biology|0": {
            "acc": 0.24305555555555555,
            "acc_stderr": 0.03586879280080341,
            "acc_norm": 0.24305555555555555,
            "acc_norm_stderr": 0.03586879280080341
        },
        "harness|winogrande|0": {
            "acc": 0.49013417521704816,
            "acc_stderr": 0.014049749833367592
        },
        "harness|hendrycksTest-clinical_knowledge|0": {
            "acc": 0.2792452830188679,
            "acc_stderr": 0.027611163402399715,
            "acc_norm": 0.2792452830188679,
            "acc_norm_stderr": 0.027611163402399715
        },
        "harness|hendrycksTest-world_religions|0": {
            "acc": 0.21052631578947367,
            "acc_stderr": 0.0312678171466318,
            "acc_norm": 0.21052631578947367,
            "acc_norm_stderr": 0.0312678171466318
        },
        "harness|hendrycksTest-college_medicine|0": {
            "acc": 0.30057803468208094,
            "acc_stderr": 0.03496101481191182,
            "acc_norm": 0.30057803468208094,
            "acc_norm_stderr": 0.03496101481191182
        },
        "harness|hendrycksTest-professional_psychology|0": {
            "acc": 0.2434640522875817,
            "acc_stderr": 0.017362473762146637,
            "acc_norm": 0.2434640522875817,
            "acc_norm_stderr": 0.017362473762146637
        },
        "harness|hendrycksTest-professional_accounting|0": {
            "acc": 0.2695035460992908,
            "acc_stderr": 0.02646903681859063,
            "acc_norm": 0.2695035460992908,
            "acc_norm_stderr": 0.02646903681859063
        },
        "harness|hendrycksTest-elementary_mathematics|0": {
            "acc": 0.24603174603174602,
            "acc_stderr": 0.022182037202948368,
            "acc_norm": 0.24603174603174602,
            "acc_norm_stderr": 0.022182037202948368
        },
        "harness|arc:easy|0": {
            "acc": 0.3463804713804714,
            "acc_stderr": 0.00976354207569573,
            "acc_norm": 0.33164983164983164,
            "acc_norm_stderr": 0.009660733780923967
        },
        "harness|hendrycksTest-high_school_computer_science|0": {
            "acc": 0.19,
            "acc_stderr": 0.039427724440366234,
            "acc_norm": 0.19,
            "acc_norm_stderr": 0.039427724440366234
        },
        "harness|hendrycksTest-nutrition|0": {
            "acc": 0.19607843137254902,
            "acc_stderr": 0.022733789405447617,
            "acc_norm": 0.19607843137254902,
            "acc_norm_stderr": 0.022733789405447617
        },
        "harness|hendrycksTest-public_relations|0": {
            "acc": 0.2818181818181818,
            "acc_stderr": 0.0430911870994646,
            "acc_norm": 0.2818181818181818,
            "acc_norm_stderr": 0.0430911870994646
        },
        "harness|lambada:openai|0": {
            "ppl": 9226.481938538587,
            "ppl_stderr": 510.3925238957856,
            "acc": 0.04366388511546672,
            "acc_stderr": 0.0028469414724731725
        },
        "harness|hendrycksTest-us_foreign_policy|0": {
            "acc": 0.29,
            "acc_stderr": 0.04560480215720684,
            "acc_norm": 0.29,
            "acc_norm_stderr": 0.04560480215720684
        },
        "harness|hendrycksTest-high_school_geography|0": {
            "acc": 0.2222222222222222,
            "acc_stderr": 0.029620227874790486,
            "acc_norm": 0.2222222222222222,
            "acc_norm_stderr": 0.029620227874790486
        },
        "harness|hendrycksTest-marketing|0": {
            "acc": 0.25213675213675213,
            "acc_stderr": 0.02844796547623101,
            "acc_norm": 0.25213675213675213,
            "acc_norm_stderr": 0.02844796547623101
        },
        "harness|hendrycksTest-medical_genetics|0": {
            "acc": 0.26,
            "acc_stderr": 0.044084400227680794,
            "acc_norm": 0.26,
            "acc_norm_stderr": 0.044084400227680794
        },
        "harness|hellaswag|0": {
            "acc": 0.2735510854411472,
            "acc_stderr": 0.004448701611795094,
            "acc_norm": 0.29486158135829516,
            "acc_norm_stderr": 0.004550486186019074
        },
        "harness|hendrycksTest-global_facts|0": {
            "acc": 0.31,
            "acc_stderr": 0.04648231987117316,
            "acc_norm": 0.31,
            "acc_norm_stderr": 0.04648231987117316
        },
        "harness|hendrycksTest-philosophy|0": {
            "acc": 0.21864951768488747,
            "acc_stderr": 0.0234755814178611,
            "acc_norm": 0.21864951768488747,
            "acc_norm_stderr": 0.0234755814178611
        },
        "harness|hendrycksTest-professional_medicine|0": {
            "acc": 0.22058823529411764,
            "acc_stderr": 0.02518778666022726,
            "acc_norm": 0.22058823529411764,
            "acc_norm_stderr": 0.02518778666022726
        },
        "harness|boolq|0": {
            "acc": 0.43119266055045874,
            "acc_stderr": 0.008661853128165599
        },
        "harness|hendrycksTest-sociology|0": {
            "acc": 0.24875621890547264,
            "acc_stderr": 0.030567675938916704,
            "acc_norm": 0.24875621890547264,
            "acc_norm_stderr": 0.030567675938916704
        },
        "harness|hendrycksTest-high_school_european_history|0": {
            "acc": 0.2909090909090909,
            "acc_stderr": 0.03546563019624336,
            "acc_norm": 0.2909090909090909,
            "acc_norm_stderr": 0.03546563019624336
        },
        "harness|hendrycksTest-business_ethics|0": {
            "acc": 0.31,
            "acc_stderr": 0.04648231987117316,
            "acc_norm": 0.31,
            "acc_norm_stderr": 0.04648231987117316
        },
        "harness|hendrycksTest-high_school_mathematics|0": {
            "acc": 0.23703703703703705,
            "acc_stderr": 0.025928876132766093,
            "acc_norm": 0.23703703703703705,
            "acc_norm_stderr": 0.025928876132766093
        },
        "harness|arc:challenge|0": {
            "acc": 0.18515358361774745,
            "acc_stderr": 0.011350774438389702,
            "acc_norm": 0.2175767918088737,
            "acc_norm_stderr": 0.012057262020972504
        },
        "harness|hendrycksTest-logical_fallacies|0": {
            "acc": 0.2085889570552147,
            "acc_stderr": 0.031921934489347235,
            "acc_norm": 0.2085889570552147,
            "acc_norm_stderr": 0.031921934489347235
        },
        "harness|hendrycksTest-prehistory|0": {
            "acc": 0.25,
            "acc_stderr": 0.02409347123262133,
            "acc_norm": 0.25,
            "acc_norm_stderr": 0.02409347123262133
        },
        "harness|hendrycksTest-electrical_engineering|0": {
            "acc": 0.2482758620689655,
            "acc_stderr": 0.03600105692727772,
            "acc_norm": 0.2482758620689655,
            "acc_norm_stderr": 0.03600105692727772
        },
        "harness|hendrycksTest-astronomy|0": {
            "acc": 0.17105263157894737,
            "acc_stderr": 0.030643607071677088,
            "acc_norm": 0.17105263157894737,
            "acc_norm_stderr": 0.030643607071677088
        },
        "harness|hendrycksTest-college_computer_science|0": {
            "acc": 0.15,
            "acc_stderr": 0.035887028128263714,
            "acc_norm": 0.15,
            "acc_norm_stderr": 0.035887028128263714
        },
        "harness|hendrycksTest-high_school_microeconomics|0": {
            "acc": 0.22268907563025211,
            "acc_stderr": 0.027025433498882385,
            "acc_norm": 0.22268907563025211,
            "acc_norm_stderr": 0.027025433498882385
        },
        "harness|hendrycksTest-high_school_government_and_politics|0": {
            "acc": 0.22797927461139897,
            "acc_stderr": 0.030276909945178256,
            "acc_norm": 0.22797927461139897,
            "acc_norm_stderr": 0.030276909945178256
        },
        "harness|hendrycksTest-high_school_chemistry|0": {
            "acc": 0.2561576354679803,
            "acc_stderr": 0.030712730070982592,
            "acc_norm": 0.2561576354679803,
            "acc_norm_stderr": 0.030712730070982592
        },
        "harness|hendrycksTest-high_school_us_history|0": {
            "acc": 0.22549019607843138,
            "acc_stderr": 0.029331162294251735,
            "acc_norm": 0.22549019607843138,
            "acc_norm_stderr": 0.029331162294251735
        },
        "harness|hendrycksTest-high_school_biology|0": {
            "acc": 0.23548387096774193,
            "acc_stderr": 0.02413763242933771,
            "acc_norm": 0.23548387096774193,
            "acc_norm_stderr": 0.02413763242933771
        },
        "harness|hendrycksTest-conceptual_physics|0": {
            "acc": 0.31063829787234043,
            "acc_stderr": 0.03025123757921317,
            "acc_norm": 0.31063829787234043,
            "acc_norm_stderr": 0.03025123757921317
        },
        "harness|hendrycksTest-international_law|0": {
            "acc": 0.2892561983471074,
            "acc_stderr": 0.041391127276354626,
            "acc_norm": 0.2892561983471074,
            "acc_norm_stderr": 0.041391127276354626
        },
        "harness|hendrycksTest-moral_disputes|0": {
            "acc": 0.27167630057803466,
            "acc_stderr": 0.02394851290546835,
            "acc_norm": 0.27167630057803466,
            "acc_norm_stderr": 0.02394851290546835
        },
        "harness|hendrycksTest-anatomy|0": {
            "acc": 0.28888888888888886,
            "acc_stderr": 0.03915450630414251,
            "acc_norm": 0.28888888888888886,
            "acc_norm_stderr": 0.03915450630414251
        },
        "harness|piqa|0": {
            "acc": 0.573993471164309,
            "acc_stderr": 0.011537375448519445,
            "acc_norm": 0.558215451577802,
            "acc_norm_stderr": 0.011586482494310216
        },
        "harness|hendrycksTest-college_mathematics|0": {
            "acc": 0.21,
            "acc_stderr": 0.04093601807403325,
            "acc_norm": 0.21,
            "acc_norm_stderr": 0.04093601807403325
        },
        "harness|hendrycksTest-high_school_world_history|0": {
            "acc": 0.26582278481012656,
            "acc_stderr": 0.02875679962965834,
            "acc_norm": 0.26582278481012656,
            "acc_norm_stderr": 0.02875679962965834
        },
        "harness|hendrycksTest-machine_learning|0": {
            "acc": 0.26785714285714285,
            "acc_stderr": 0.04203277291467763,
            "acc_norm": 0.26785714285714285,
            "acc_norm_stderr": 0.04203277291467763
        },
        "harness|truthfulqa:mc|0": {
            "mc1": 0.23990208078335373,
            "mc1_stderr": 0.01494881267906214,
            "mc2": 0.4790316610491991,
            "mc2_stderr": 0.016236266265697782
        },
        "harness|hendrycksTest-security_studies|0": {
            "acc": 0.22448979591836735,
            "acc_stderr": 0.02671143055553841,
            "acc_norm": 0.22448979591836735,
            "acc_norm_stderr": 0.02671143055553841
        },
        "harness|hendrycksTest-jurisprudence|0": {
            "acc": 0.3148148148148148,
            "acc_stderr": 0.04489931073591312,
            "acc_norm": 0.3148148148148148,
            "acc_norm_stderr": 0.04489931073591312
        },
        "harness|hendrycksTest-abstract_algebra|0": {
            "acc": 0.29,
            "acc_stderr": 0.04560480215720683,
            "acc_norm": 0.29,
            "acc_norm_stderr": 0.04560480215720683
        },
        "harness|openbookqa|0": {
            "acc": 0.144,
            "acc_stderr": 0.015716934945725767,
            "acc_norm": 0.284,
            "acc_norm_stderr": 0.020186703693570847
        },
        "harness|hendrycksTest-high_school_macroeconomics|0": {
            "acc": 0.23846153846153847,
            "acc_stderr": 0.021606294494647727,
            "acc_norm": 0.23846153846153847,
            "acc_norm_stderr": 0.021606294494647727
        },
        "harness|hendrycksTest-management|0": {
            "acc": 0.22330097087378642,
            "acc_stderr": 0.04123553189891431,
            "acc_norm": 0.22330097087378642,
            "acc_norm_stderr": 0.04123553189891431
        },
        "harness|hendrycksTest-high_school_psychology|0": {
            "acc": 0.20550458715596331,
            "acc_stderr": 0.017324352325016015,
            "acc_norm": 0.20550458715596331,
            "acc_norm_stderr": 0.017324352325016015
        }
    },
    "task_info": {
        "model": "marcsun13/opt-350m-gptq-4bit",
        "revision": "main",
        "private": false,
        "params": 0.35,
        "architectures": "OPTForCausalLM",
        "quant_type": "GPTQ",
        "precision": "4bit",
        "weight_dtype": "int4",
        "compute_dtype": "bfloat16",
        "gguf_ftype": "*Q4_0.gguf",
        "hardware": "cpu",
        "status": "Pending",
        "submitted_time": "2024-04-22T04:30:37Z",
        "model_type": "quantization",
        "job_id": -1,
        "job_start_time": null,
        "scripts": "ITREX"
    },
    "quantization_config": {
        "batch_size": 1,
        "bits": 4,
        "block_name_to_quantize": "model.decoder.layers",
        "damp_percent": 0.01,
        "dataset": [
            "auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm."
        ],
        "desc_act": false,
        "group_size": 128,
        "model_seqlen": 2048,
        "module_name_preceding_first_block": [
            "model.decoder.embed_tokens",
            "model.decoder.embed_positions",
            "model.decoder.final_layer_norm"
        ],
        "pack_sequentially": false,
        "pad_token_id": null,
        "quant_method": "gptq",
        "sym": true,
        "tokenizer": null,
        "true_sequential": true,
        "use_cuda_fp16": true,
        "disable_exllama": true
    },
    "versions": {
        "harness|hendrycksTest-virology|0": 1,
        "harness|hendrycksTest-miscellaneous|0": 1,
        "harness|hendrycksTest-human_sexuality|0": 1,
        "harness|hendrycksTest-econometrics|0": 1,
        "harness|hendrycksTest-college_physics|0": 1,
        "harness|hendrycksTest-high_school_physics|0": 1,
        "harness|hendrycksTest-college_chemistry|0": 1,
        "harness|hendrycksTest-formal_logic|0": 1,
        "harness|hendrycksTest-moral_scenarios|0": 1,
        "harness|hendrycksTest-professional_law|0": 1,
        "harness|hendrycksTest-human_aging|0": 1,
        "harness|hendrycksTest-computer_security|0": 1,
        "harness|hendrycksTest-high_school_statistics|0": 1,
        "harness|hendrycksTest-college_biology|0": 1,
        "harness|winogrande|0": 0,
        "harness|hendrycksTest-clinical_knowledge|0": 1,
        "harness|hendrycksTest-world_religions|0": 1,
        "harness|hendrycksTest-college_medicine|0": 1,
        "harness|hendrycksTest-professional_psychology|0": 1,
        "harness|hendrycksTest-professional_accounting|0": 1,
        "harness|hendrycksTest-elementary_mathematics|0": 1,
        "harness|arc:easy|0": 0,
        "harness|hendrycksTest-high_school_computer_science|0": 1,
        "harness|hendrycksTest-nutrition|0": 1,
        "harness|hendrycksTest-public_relations|0": 1,
        "harness|lambada:openai|0": 0,
        "harness|hendrycksTest-us_foreign_policy|0": 1,
        "harness|hendrycksTest-high_school_geography|0": 1,
        "harness|hendrycksTest-marketing|0": 1,
        "harness|hendrycksTest-medical_genetics|0": 1,
        "harness|hellaswag|0": 0,
        "harness|hendrycksTest-global_facts|0": 1,
        "harness|hendrycksTest-philosophy|0": 1,
        "harness|hendrycksTest-professional_medicine|0": 1,
        "harness|boolq|0": 1,
        "harness|hendrycksTest-sociology|0": 1,
        "harness|hendrycksTest-high_school_european_history|0": 1,
        "harness|hendrycksTest-business_ethics|0": 1,
        "harness|hendrycksTest-high_school_mathematics|0": 1,
        "harness|arc:challenge|0": 0,
        "harness|hendrycksTest-logical_fallacies|0": 1,
        "harness|hendrycksTest-prehistory|0": 1,
        "harness|hendrycksTest-electrical_engineering|0": 1,
        "harness|hendrycksTest-astronomy|0": 1,
        "harness|hendrycksTest-college_computer_science|0": 1,
        "harness|hendrycksTest-high_school_microeconomics|0": 1,
        "harness|hendrycksTest-high_school_government_and_politics|0": 1,
        "harness|hendrycksTest-high_school_chemistry|0": 1,
        "harness|hendrycksTest-high_school_us_history|0": 1,
        "harness|hendrycksTest-high_school_biology|0": 1,
        "harness|hendrycksTest-conceptual_physics|0": 1,
        "harness|hendrycksTest-international_law|0": 1,
        "harness|hendrycksTest-moral_disputes|0": 1,
        "harness|hendrycksTest-anatomy|0": 1,
        "harness|piqa|0": 0,
        "harness|hendrycksTest-college_mathematics|0": 1,
        "harness|hendrycksTest-high_school_world_history|0": 1,
        "harness|hendrycksTest-machine_learning|0": 1,
        "harness|truthfulqa:mc|0": 1,
        "harness|hendrycksTest-security_studies|0": 1,
        "harness|hendrycksTest-jurisprudence|0": 1,
        "harness|hendrycksTest-abstract_algebra|0": 1,
        "harness|openbookqa|0": 0,
        "harness|hendrycksTest-high_school_macroeconomics|0": 1,
        "harness|hendrycksTest-management|0": 1,
        "harness|hendrycksTest-high_school_psychology|0": 1
    }
}