{
    "config_general": {
        "lighteval_sha": "1.14",
        "num_few_shot_default": null,
        "num_fewshot_seeds": null,
        "override_batch_size": null,
        "max_samples": null,
        "job_id": -1,
        "start_time": null,
        "end_time": "2024-04-17-08-55-48",
        "total_evaluation_time_secondes": "",
        "model_name": "marcsun13/opt-350m-gptq-4bit",
        "model_sha": "",
        "model_dtype": "4bit",
        "model_size": 2.8
    },
    "results": {
        "harness|winogrande|0": {
            "acc": 0.49013417521704816,
            "acc_stderr": 0.014049749833367592
        },
        "harness|arc:easy|0": {
            "acc": 0.3463804713804714,
            "acc_stderr": 0.00976354207569573,
            "acc_norm": 0.33164983164983164,
            "acc_norm_stderr": 0.009660733780923967
        },
        "harness|arc:challenge|0": {
            "acc": 0.18515358361774745,
            "acc_stderr": 0.011350774438389702,
            "acc_norm": 0.2175767918088737,
            "acc_norm_stderr": 0.012057262020972504
        },
        "harness|truthfulqa:mc|0": {
            "mc1": 0.23990208078335373,
            "mc1_stderr": 0.01494881267906214,
            "mc2": 0.4790316610491991,
            "mc2_stderr": 0.016236266265697782
        }
    },
    "task_info": {
        "model": "marcsun13/opt-350m-gptq-4bit",
        "revision": "main",
        "private": false,
        "params": 2.8,
        "architectures": "OPTForCausalLM",
        "quant_type": "GPTQ",
        "precision": "4bit",
        "weight_dtype": "int4",
        "compute_dtype": "bfloat16",
        "hardware": "cpu",
        "status": "PENDING",
        "submitted_time": "2024-04-17T08:13:29Z",
        "model_type": "quantization",
        "job_id": -1,
        "job_start_time": null,
        "scripts": "ITREX"
    },
    "quantization_config": {
        "batch_size": 1,
        "bits": 4,
        "block_name_to_quantize": "model.decoder.layers",
        "damp_percent": 0.01,
        "dataset": [
            "auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm."
        ],
        "desc_act": false,
        "group_size": 128,
        "model_seqlen": 2048,
        "module_name_preceding_first_block": [
            "model.decoder.embed_tokens",
            "model.decoder.embed_positions",
            "model.decoder.final_layer_norm"
        ],
        "pack_sequentially": false,
        "pad_token_id": null,
        "quant_method": "gptq",
        "sym": true,
        "tokenizer": null,
        "true_sequential": true,
        "use_cuda_fp16": true,
        "disable_exllama": true
    },
    "versions": {
        "harness|winogrande|0": 0,
        "harness|arc:easy|0": 0,
        "harness|arc:challenge|0": 0,
        "harness|truthfulqa:mc|0": 1
    }
}