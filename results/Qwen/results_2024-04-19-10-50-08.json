{
    "config_general": {
        "lighteval_sha": "1.4",
        "num_few_shot_default": null,
        "num_fewshot_seeds": null,
        "override_batch_size": null,
        "max_samples": null,
        "job_id": -1,
        "start_time": null,
        "end_time": "2024-04-19-10-50-08",
        "total_evaluation_time_secondes": "",
        "model_name": "Qwen/Qwen1.5-0.5B-Chat-GPTQ-Int4",
        "model_sha": "",
        "model_dtype": "4bit",
        "model_size": 0.197
    },
    "results": {
        "harness|openbookqa|0": {
            "acc": 0.196,
            "acc_stderr": 0.017770751227744873,
            "acc_norm": 0.302,
            "acc_norm_stderr": 0.02055326917420918
        },
        "harness|hendrycksTest-college_biology|0": {
            "acc": 0.3055555555555556,
            "acc_stderr": 0.03852084696008534,
            "acc_norm": 0.3055555555555556,
            "acc_norm_stderr": 0.03852084696008534
        },
        "harness|hendrycksTest-high_school_chemistry|0": {
            "acc": 0.18719211822660098,
            "acc_stderr": 0.027444924966882618,
            "acc_norm": 0.18719211822660098,
            "acc_norm_stderr": 0.027444924966882618
        },
        "harness|hendrycksTest-clinical_knowledge|0": {
            "acc": 0.2981132075471698,
            "acc_stderr": 0.028152837942493857,
            "acc_norm": 0.2981132075471698,
            "acc_norm_stderr": 0.028152837942493857
        },
        "harness|hendrycksTest-computer_security|0": {
            "acc": 0.32,
            "acc_stderr": 0.046882617226215034,
            "acc_norm": 0.32,
            "acc_norm_stderr": 0.046882617226215034
        },
        "harness|hendrycksTest-world_religions|0": {
            "acc": 0.34502923976608185,
            "acc_stderr": 0.036459813773888065,
            "acc_norm": 0.34502923976608185,
            "acc_norm_stderr": 0.036459813773888065
        },
        "harness|hendrycksTest-high_school_computer_science|0": {
            "acc": 0.36,
            "acc_stderr": 0.04824181513244218,
            "acc_norm": 0.36,
            "acc_norm_stderr": 0.04824181513244218
        },
        "harness|hendrycksTest-elementary_mathematics|0": {
            "acc": 0.2777777777777778,
            "acc_stderr": 0.02306818884826112,
            "acc_norm": 0.2777777777777778,
            "acc_norm_stderr": 0.02306818884826112
        },
        "harness|hendrycksTest-conceptual_physics|0": {
            "acc": 0.2765957446808511,
            "acc_stderr": 0.029241883869628827,
            "acc_norm": 0.2765957446808511,
            "acc_norm_stderr": 0.029241883869628827
        },
        "harness|hendrycksTest-high_school_european_history|0": {
            "acc": 0.43636363636363634,
            "acc_stderr": 0.03872592983524753,
            "acc_norm": 0.43636363636363634,
            "acc_norm_stderr": 0.03872592983524753
        },
        "harness|hendrycksTest-jurisprudence|0": {
            "acc": 0.4074074074074074,
            "acc_stderr": 0.04750077341199986,
            "acc_norm": 0.4074074074074074,
            "acc_norm_stderr": 0.04750077341199986
        },
        "harness|hendrycksTest-logical_fallacies|0": {
            "acc": 0.37423312883435583,
            "acc_stderr": 0.03802068102899615,
            "acc_norm": 0.37423312883435583,
            "acc_norm_stderr": 0.03802068102899615
        },
        "harness|hellaswag|0": {
            "acc": 0.3573989245170285,
            "acc_stderr": 0.004782542754102077,
            "acc_norm": 0.43995220075682134,
            "acc_norm_stderr": 0.004953667028654386
        },
        "harness|hendrycksTest-public_relations|0": {
            "acc": 0.32727272727272727,
            "acc_stderr": 0.04494290866252089,
            "acc_norm": 0.32727272727272727,
            "acc_norm_stderr": 0.04494290866252089
        },
        "harness|hendrycksTest-high_school_microeconomics|0": {
            "acc": 0.2605042016806723,
            "acc_stderr": 0.028510251512341944,
            "acc_norm": 0.2605042016806723,
            "acc_norm_stderr": 0.028510251512341944
        },
        "harness|hendrycksTest-high_school_macroeconomics|0": {
            "acc": 0.258974358974359,
            "acc_stderr": 0.022211106810061675,
            "acc_norm": 0.258974358974359,
            "acc_norm_stderr": 0.022211106810061675
        },
        "harness|hendrycksTest-miscellaneous|0": {
            "acc": 0.3448275862068966,
            "acc_stderr": 0.016997123346113432,
            "acc_norm": 0.3448275862068966,
            "acc_norm_stderr": 0.016997123346113432
        },
        "harness|arc:easy|0": {
            "acc": 0.5214646464646465,
            "acc_stderr": 0.01025032515945665,
            "acc_norm": 0.47685185185185186,
            "acc_norm_stderr": 0.010248782484554473
        },
        "harness|hendrycksTest-marketing|0": {
            "acc": 0.4358974358974359,
            "acc_stderr": 0.032485775115784,
            "acc_norm": 0.4358974358974359,
            "acc_norm_stderr": 0.032485775115784
        },
        "harness|hendrycksTest-global_facts|0": {
            "acc": 0.26,
            "acc_stderr": 0.0440844002276808,
            "acc_norm": 0.26,
            "acc_norm_stderr": 0.0440844002276808
        },
        "harness|hendrycksTest-high_school_statistics|0": {
            "acc": 0.18055555555555555,
            "acc_stderr": 0.026232878971491652,
            "acc_norm": 0.18055555555555555,
            "acc_norm_stderr": 0.026232878971491652
        },
        "harness|hendrycksTest-anatomy|0": {
            "acc": 0.2740740740740741,
            "acc_stderr": 0.03853254836552003,
            "acc_norm": 0.2740740740740741,
            "acc_norm_stderr": 0.03853254836552003
        },
        "harness|hendrycksTest-electrical_engineering|0": {
            "acc": 0.32413793103448274,
            "acc_stderr": 0.03900432069185555,
            "acc_norm": 0.32413793103448274,
            "acc_norm_stderr": 0.03900432069185555
        },
        "harness|hendrycksTest-high_school_psychology|0": {
            "acc": 0.3302752293577982,
            "acc_stderr": 0.02016446633634298,
            "acc_norm": 0.3302752293577982,
            "acc_norm_stderr": 0.02016446633634298
        },
        "harness|hendrycksTest-prehistory|0": {
            "acc": 0.30864197530864196,
            "acc_stderr": 0.025702640260603746,
            "acc_norm": 0.30864197530864196,
            "acc_norm_stderr": 0.025702640260603746
        },
        "harness|hendrycksTest-human_sexuality|0": {
            "acc": 0.37404580152671757,
            "acc_stderr": 0.04243869242230524,
            "acc_norm": 0.37404580152671757,
            "acc_norm_stderr": 0.04243869242230524
        },
        "harness|arc:challenge|0": {
            "acc": 0.2551194539249147,
            "acc_stderr": 0.012739038695202107,
            "acc_norm": 0.27474402730375425,
            "acc_norm_stderr": 0.013044617212771227
        },
        "harness|hendrycksTest-high_school_mathematics|0": {
            "acc": 0.23333333333333334,
            "acc_stderr": 0.025787874220959288,
            "acc_norm": 0.23333333333333334,
            "acc_norm_stderr": 0.025787874220959288
        },
        "harness|hendrycksTest-us_foreign_policy|0": {
            "acc": 0.41,
            "acc_stderr": 0.04943110704237102,
            "acc_norm": 0.41,
            "acc_norm_stderr": 0.04943110704237102
        },
        "harness|hendrycksTest-high_school_physics|0": {
            "acc": 0.2119205298013245,
            "acc_stderr": 0.03336767086567977,
            "acc_norm": 0.2119205298013245,
            "acc_norm_stderr": 0.03336767086567977
        },
        "harness|hendrycksTest-international_law|0": {
            "acc": 0.38016528925619836,
            "acc_stderr": 0.04431324501968432,
            "acc_norm": 0.38016528925619836,
            "acc_norm_stderr": 0.04431324501968432
        },
        "harness|hendrycksTest-college_medicine|0": {
            "acc": 0.27167630057803466,
            "acc_stderr": 0.0339175032232166,
            "acc_norm": 0.27167630057803466,
            "acc_norm_stderr": 0.0339175032232166
        },
        "harness|hendrycksTest-college_chemistry|0": {
            "acc": 0.21,
            "acc_stderr": 0.04093601807403326,
            "acc_norm": 0.21,
            "acc_norm_stderr": 0.04093601807403326
        },
        "harness|hendrycksTest-formal_logic|0": {
            "acc": 0.29365079365079366,
            "acc_stderr": 0.04073524322147124,
            "acc_norm": 0.29365079365079366,
            "acc_norm_stderr": 0.04073524322147124
        },
        "harness|hendrycksTest-high_school_geography|0": {
            "acc": 0.2828282828282828,
            "acc_stderr": 0.03208779558786749,
            "acc_norm": 0.2828282828282828,
            "acc_norm_stderr": 0.03208779558786749
        },
        "harness|hendrycksTest-high_school_world_history|0": {
            "acc": 0.4345991561181435,
            "acc_stderr": 0.03226759995510145,
            "acc_norm": 0.4345991561181435,
            "acc_norm_stderr": 0.03226759995510145
        },
        "harness|hendrycksTest-college_physics|0": {
            "acc": 0.22549019607843138,
            "acc_stderr": 0.041583075330832865,
            "acc_norm": 0.22549019607843138,
            "acc_norm_stderr": 0.041583075330832865
        },
        "harness|hendrycksTest-human_aging|0": {
            "acc": 0.3721973094170404,
            "acc_stderr": 0.03244305283008731,
            "acc_norm": 0.3721973094170404,
            "acc_norm_stderr": 0.03244305283008731
        },
        "harness|hendrycksTest-professional_accounting|0": {
            "acc": 0.2553191489361702,
            "acc_stderr": 0.026011992930902013,
            "acc_norm": 0.2553191489361702,
            "acc_norm_stderr": 0.026011992930902013
        },
        "harness|hendrycksTest-business_ethics|0": {
            "acc": 0.38,
            "acc_stderr": 0.04878317312145632,
            "acc_norm": 0.38,
            "acc_norm_stderr": 0.04878317312145632
        },
        "harness|hendrycksTest-nutrition|0": {
            "acc": 0.3300653594771242,
            "acc_stderr": 0.026925654653615686,
            "acc_norm": 0.3300653594771242,
            "acc_norm_stderr": 0.026925654653615686
        },
        "harness|hendrycksTest-high_school_government_and_politics|0": {
            "acc": 0.22279792746113988,
            "acc_stderr": 0.03003114797764154,
            "acc_norm": 0.22279792746113988,
            "acc_norm_stderr": 0.03003114797764154
        },
        "harness|hendrycksTest-high_school_us_history|0": {
            "acc": 0.3333333333333333,
            "acc_stderr": 0.033086111132364336,
            "acc_norm": 0.3333333333333333,
            "acc_norm_stderr": 0.033086111132364336
        },
        "harness|hendrycksTest-virology|0": {
            "acc": 0.3373493975903614,
            "acc_stderr": 0.03680783690727581,
            "acc_norm": 0.3373493975903614,
            "acc_norm_stderr": 0.03680783690727581
        },
        "harness|hendrycksTest-security_studies|0": {
            "acc": 0.19591836734693877,
            "acc_stderr": 0.025409301953225678,
            "acc_norm": 0.19591836734693877,
            "acc_norm_stderr": 0.025409301953225678
        },
        "harness|hendrycksTest-machine_learning|0": {
            "acc": 0.2857142857142857,
            "acc_stderr": 0.04287858751340456,
            "acc_norm": 0.2857142857142857,
            "acc_norm_stderr": 0.04287858751340456
        },
        "harness|lambada:openai|0": {
            "ppl": 32.87142888449699,
            "ppl_stderr": 1.6164495010723576,
            "acc": 0.3980205705414322,
            "acc_stderr": 0.006819549047940035
        },
        "harness|hendrycksTest-philosophy|0": {
            "acc": 0.3279742765273312,
            "acc_stderr": 0.026664410886937606,
            "acc_norm": 0.3279742765273312,
            "acc_norm_stderr": 0.026664410886937606
        },
        "harness|hendrycksTest-management|0": {
            "acc": 0.3786407766990291,
            "acc_stderr": 0.04802694698258974,
            "acc_norm": 0.3786407766990291,
            "acc_norm_stderr": 0.04802694698258974
        },
        "harness|hendrycksTest-econometrics|0": {
            "acc": 0.21052631578947367,
            "acc_stderr": 0.03835153954399421,
            "acc_norm": 0.21052631578947367,
            "acc_norm_stderr": 0.03835153954399421
        },
        "harness|hendrycksTest-moral_disputes|0": {
            "acc": 0.3439306358381503,
            "acc_stderr": 0.025574123786546648,
            "acc_norm": 0.3439306358381503,
            "acc_norm_stderr": 0.025574123786546648
        },
        "harness|boolq|0": {
            "acc": 0.41039755351681956,
            "acc_stderr": 0.00860348804861752
        },
        "harness|hendrycksTest-college_computer_science|0": {
            "acc": 0.35,
            "acc_stderr": 0.0479372485441102,
            "acc_norm": 0.35,
            "acc_norm_stderr": 0.0479372485441102
        },
        "harness|winogrande|0": {
            "acc": 0.5469613259668509,
            "acc_stderr": 0.013990366632148105
        },
        "harness|hendrycksTest-professional_medicine|0": {
            "acc": 0.1801470588235294,
            "acc_stderr": 0.02334516361654485,
            "acc_norm": 0.1801470588235294,
            "acc_norm_stderr": 0.02334516361654485
        },
        "harness|hendrycksTest-sociology|0": {
            "acc": 0.39800995024875624,
            "acc_stderr": 0.034611994290400135,
            "acc_norm": 0.39800995024875624,
            "acc_norm_stderr": 0.034611994290400135
        },
        "harness|hendrycksTest-professional_psychology|0": {
            "acc": 0.27941176470588236,
            "acc_stderr": 0.018152871051538826,
            "acc_norm": 0.27941176470588236,
            "acc_norm_stderr": 0.018152871051538826
        },
        "harness|hendrycksTest-abstract_algebra|0": {
            "acc": 0.25,
            "acc_stderr": 0.04351941398892446,
            "acc_norm": 0.25,
            "acc_norm_stderr": 0.04351941398892446
        },
        "harness|piqa|0": {
            "acc": 0.6735582154515778,
            "acc_stderr": 0.010940467046177297,
            "acc_norm": 0.6632208922742111,
            "acc_norm_stderr": 0.01102673892525118
        },
        "harness|hendrycksTest-medical_genetics|0": {
            "acc": 0.39,
            "acc_stderr": 0.04902071300001975,
            "acc_norm": 0.39,
            "acc_norm_stderr": 0.04902071300001975
        },
        "harness|hendrycksTest-professional_law|0": {
            "acc": 0.29465449804432853,
            "acc_stderr": 0.011643576764069552,
            "acc_norm": 0.29465449804432853,
            "acc_norm_stderr": 0.011643576764069552
        },
        "harness|truthfulqa:mc|0": {
            "mc1": 0.26193390452876375,
            "mc1_stderr": 0.01539211880501503,
            "mc2": 0.42466676187425106,
            "mc2_stderr": 0.01508193165612733
        },
        "harness|hendrycksTest-college_mathematics|0": {
            "acc": 0.3,
            "acc_stderr": 0.046056618647183814,
            "acc_norm": 0.3,
            "acc_norm_stderr": 0.046056618647183814
        },
        "harness|hendrycksTest-moral_scenarios|0": {
            "acc": 0.23798882681564246,
            "acc_stderr": 0.014242630070574915,
            "acc_norm": 0.23798882681564246,
            "acc_norm_stderr": 0.014242630070574915
        },
        "harness|hendrycksTest-high_school_biology|0": {
            "acc": 0.27419354838709675,
            "acc_stderr": 0.0253781399708852,
            "acc_norm": 0.27419354838709675,
            "acc_norm_stderr": 0.0253781399708852
        },
        "harness|hendrycksTest-astronomy|0": {
            "acc": 0.23684210526315788,
            "acc_stderr": 0.03459777606810538,
            "acc_norm": 0.23684210526315788,
            "acc_norm_stderr": 0.03459777606810538
        }
    },
    "task_info": {
        "model": "Qwen/Qwen1.5-0.5B-Chat-GPTQ-Int4",
        "revision": "main",
        "private": false,
        "params": 0.197,
        "architectures": "Qwen2ForCausalLM",
        "quant_type": "GPTQ",
        "precision": "4bit",
        "weight_dtype": "int4",
        "compute_dtype": "bfloat16",
        "hardware": "cpu",
        "status": "Pending",
        "submitted_time": "2024-04-19T05:06:08Z",
        "model_type": "quantization",
        "job_id": -1,
        "job_start_time": null,
        "scripts": "ITREX"
    },
    "quantization_config": {
        "batch_size": 1,
        "bits": 4,
        "block_name_to_quantize": null,
        "cache_block_outputs": true,
        "damp_percent": 0.01,
        "dataset": null,
        "desc_act": false,
        "exllama_config": {
            "version": 1
        },
        "group_size": 128,
        "max_input_length": null,
        "model_seqlen": null,
        "module_name_preceding_first_block": null,
        "modules_in_block_to_quantize": null,
        "pad_token_id": null,
        "quant_method": "gptq",
        "sym": true,
        "tokenizer": null,
        "true_sequential": true,
        "use_cuda_fp16": false,
        "use_exllama": true,
        "disable_exllama": true
    },
    "versions": {
        "harness|openbookqa|0": 0,
        "harness|hendrycksTest-college_biology|0": 1,
        "harness|hendrycksTest-high_school_chemistry|0": 1,
        "harness|hendrycksTest-clinical_knowledge|0": 1,
        "harness|hendrycksTest-computer_security|0": 1,
        "harness|hendrycksTest-world_religions|0": 1,
        "harness|hendrycksTest-high_school_computer_science|0": 1,
        "harness|hendrycksTest-elementary_mathematics|0": 1,
        "harness|hendrycksTest-conceptual_physics|0": 1,
        "harness|hendrycksTest-high_school_european_history|0": 1,
        "harness|hendrycksTest-jurisprudence|0": 1,
        "harness|hendrycksTest-logical_fallacies|0": 1,
        "harness|hellaswag|0": 0,
        "harness|hendrycksTest-public_relations|0": 1,
        "harness|hendrycksTest-high_school_microeconomics|0": 1,
        "harness|hendrycksTest-high_school_macroeconomics|0": 1,
        "harness|hendrycksTest-miscellaneous|0": 1,
        "harness|arc:easy|0": 0,
        "harness|hendrycksTest-marketing|0": 1,
        "harness|hendrycksTest-global_facts|0": 1,
        "harness|hendrycksTest-high_school_statistics|0": 1,
        "harness|hendrycksTest-anatomy|0": 1,
        "harness|hendrycksTest-electrical_engineering|0": 1,
        "harness|hendrycksTest-high_school_psychology|0": 1,
        "harness|hendrycksTest-prehistory|0": 1,
        "harness|hendrycksTest-human_sexuality|0": 1,
        "harness|arc:challenge|0": 0,
        "harness|hendrycksTest-high_school_mathematics|0": 1,
        "harness|hendrycksTest-us_foreign_policy|0": 1,
        "harness|hendrycksTest-high_school_physics|0": 1,
        "harness|hendrycksTest-international_law|0": 1,
        "harness|hendrycksTest-college_medicine|0": 1,
        "harness|hendrycksTest-college_chemistry|0": 1,
        "harness|hendrycksTest-formal_logic|0": 1,
        "harness|hendrycksTest-high_school_geography|0": 1,
        "harness|hendrycksTest-high_school_world_history|0": 1,
        "harness|hendrycksTest-college_physics|0": 1,
        "harness|hendrycksTest-human_aging|0": 1,
        "harness|hendrycksTest-professional_accounting|0": 1,
        "harness|hendrycksTest-business_ethics|0": 1,
        "harness|hendrycksTest-nutrition|0": 1,
        "harness|hendrycksTest-high_school_government_and_politics|0": 1,
        "harness|hendrycksTest-high_school_us_history|0": 1,
        "harness|hendrycksTest-virology|0": 1,
        "harness|hendrycksTest-security_studies|0": 1,
        "harness|hendrycksTest-machine_learning|0": 1,
        "harness|lambada:openai|0": 0,
        "harness|hendrycksTest-philosophy|0": 1,
        "harness|hendrycksTest-management|0": 1,
        "harness|hendrycksTest-econometrics|0": 1,
        "harness|hendrycksTest-moral_disputes|0": 1,
        "harness|boolq|0": 1,
        "harness|hendrycksTest-college_computer_science|0": 1,
        "harness|winogrande|0": 0,
        "harness|hendrycksTest-professional_medicine|0": 1,
        "harness|hendrycksTest-sociology|0": 1,
        "harness|hendrycksTest-professional_psychology|0": 1,
        "harness|hendrycksTest-abstract_algebra|0": 1,
        "harness|piqa|0": 0,
        "harness|hendrycksTest-medical_genetics|0": 1,
        "harness|hendrycksTest-professional_law|0": 1,
        "harness|truthfulqa:mc|0": 1,
        "harness|hendrycksTest-college_mathematics|0": 1,
        "harness|hendrycksTest-moral_scenarios|0": 1,
        "harness|hendrycksTest-high_school_biology|0": 1,
        "harness|hendrycksTest-astronomy|0": 1
    }
}