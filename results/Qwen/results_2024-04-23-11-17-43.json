{
    "config_general": {
        "lighteval_sha": "1.4",
        "num_few_shot_default": null,
        "num_fewshot_seeds": null,
        "override_batch_size": null,
        "max_samples": null,
        "job_id": -1,
        "start_time": null,
        "end_time": "2024-04-23-11-17-43",
        "total_evaluation_time_secondes": "",
        "model_name": "Qwen/Qwen1.5-7B-Chat-GPTQ-Int4",
        "model_sha": "",
        "model_dtype": "4bit",
        "model_size": 2.114
    },
    "results": {
        "harness|hendrycksTest-professional_medicine|0": {
            "acc": 0.6139705882352942,
            "acc_stderr": 0.029573269134411124,
            "acc_norm": 0.6139705882352942,
            "acc_norm_stderr": 0.029573269134411124
        },
        "harness|hendrycksTest-high_school_statistics|0": {
            "acc": 0.5185185185185185,
            "acc_stderr": 0.03407632093854051,
            "acc_norm": 0.5185185185185185,
            "acc_norm_stderr": 0.03407632093854051
        },
        "harness|hendrycksTest-management|0": {
            "acc": 0.7572815533980582,
            "acc_stderr": 0.04245022486384495,
            "acc_norm": 0.7572815533980582,
            "acc_norm_stderr": 0.04245022486384495
        },
        "harness|hendrycksTest-world_religions|0": {
            "acc": 0.7602339181286549,
            "acc_stderr": 0.03274485211946956,
            "acc_norm": 0.7602339181286549,
            "acc_norm_stderr": 0.03274485211946956
        },
        "harness|hendrycksTest-high_school_computer_science|0": {
            "acc": 0.69,
            "acc_stderr": 0.04648231987117316,
            "acc_norm": 0.69,
            "acc_norm_stderr": 0.04648231987117316
        },
        "harness|hendrycksTest-high_school_us_history|0": {
            "acc": 0.7598039215686274,
            "acc_stderr": 0.02998373305591361,
            "acc_norm": 0.7598039215686274,
            "acc_norm_stderr": 0.02998373305591361
        },
        "harness|hendrycksTest-computer_security|0": {
            "acc": 0.74,
            "acc_stderr": 0.04408440022768079,
            "acc_norm": 0.74,
            "acc_norm_stderr": 0.04408440022768079
        },
        "harness|hendrycksTest-nutrition|0": {
            "acc": 0.6699346405228758,
            "acc_stderr": 0.02692565465361569,
            "acc_norm": 0.6699346405228758,
            "acc_norm_stderr": 0.02692565465361569
        },
        "harness|hendrycksTest-clinical_knowledge|0": {
            "acc": 0.6792452830188679,
            "acc_stderr": 0.02872750295788027,
            "acc_norm": 0.6792452830188679,
            "acc_norm_stderr": 0.02872750295788027
        },
        "harness|hendrycksTest-business_ethics|0": {
            "acc": 0.65,
            "acc_stderr": 0.0479372485441102,
            "acc_norm": 0.65,
            "acc_norm_stderr": 0.0479372485441102
        },
        "harness|hendrycksTest-astronomy|0": {
            "acc": 0.6578947368421053,
            "acc_stderr": 0.038607315993160904,
            "acc_norm": 0.6578947368421053,
            "acc_norm_stderr": 0.038607315993160904
        },
        "harness|hendrycksTest-college_mathematics|0": {
            "acc": 0.35,
            "acc_stderr": 0.047937248544110196,
            "acc_norm": 0.35,
            "acc_norm_stderr": 0.047937248544110196
        },
        "harness|hendrycksTest-econometrics|0": {
            "acc": 0.5263157894736842,
            "acc_stderr": 0.046970851366478626,
            "acc_norm": 0.5263157894736842,
            "acc_norm_stderr": 0.046970851366478626
        },
        "harness|hendrycksTest-high_school_european_history|0": {
            "acc": 0.7757575757575758,
            "acc_stderr": 0.032568666616811015,
            "acc_norm": 0.7757575757575758,
            "acc_norm_stderr": 0.032568666616811015
        },
        "harness|hendrycksTest-anatomy|0": {
            "acc": 0.5555555555555556,
            "acc_stderr": 0.04292596718256981,
            "acc_norm": 0.5555555555555556,
            "acc_norm_stderr": 0.04292596718256981
        },
        "harness|hendrycksTest-medical_genetics|0": {
            "acc": 0.7,
            "acc_stderr": 0.046056618647183814,
            "acc_norm": 0.7,
            "acc_norm_stderr": 0.046056618647183814
        },
        "harness|hendrycksTest-moral_scenarios|0": {
            "acc": 0.29497206703910617,
            "acc_stderr": 0.015251931579208199,
            "acc_norm": 0.29497206703910617,
            "acc_norm_stderr": 0.015251931579208199
        },
        "harness|hendrycksTest-professional_law|0": {
            "acc": 0.45045632333767927,
            "acc_stderr": 0.012707390438502346,
            "acc_norm": 0.45045632333767927,
            "acc_norm_stderr": 0.012707390438502346
        },
        "harness|hendrycksTest-us_foreign_policy|0": {
            "acc": 0.82,
            "acc_stderr": 0.03861229196653694,
            "acc_norm": 0.82,
            "acc_norm_stderr": 0.03861229196653694
        },
        "harness|truthfulqa:mc|0": {
            "mc1": 0.3769889840881273,
            "mc1_stderr": 0.016965517578930354,
            "mc2": 0.5414045166790171,
            "mc2_stderr": 0.01585056257270011
        },
        "harness|hendrycksTest-machine_learning|0": {
            "acc": 0.39285714285714285,
            "acc_stderr": 0.04635550135609976,
            "acc_norm": 0.39285714285714285,
            "acc_norm_stderr": 0.04635550135609976
        },
        "harness|hendrycksTest-logical_fallacies|0": {
            "acc": 0.6380368098159509,
            "acc_stderr": 0.037757007291414416,
            "acc_norm": 0.6380368098159509,
            "acc_norm_stderr": 0.037757007291414416
        },
        "harness|hendrycksTest-human_sexuality|0": {
            "acc": 0.6870229007633588,
            "acc_stderr": 0.04066962905677698,
            "acc_norm": 0.6870229007633588,
            "acc_norm_stderr": 0.04066962905677698
        },
        "harness|hendrycksTest-college_computer_science|0": {
            "acc": 0.57,
            "acc_stderr": 0.04975698519562428,
            "acc_norm": 0.57,
            "acc_norm_stderr": 0.04975698519562428
        },
        "harness|hendrycksTest-international_law|0": {
            "acc": 0.7520661157024794,
            "acc_stderr": 0.03941897526516302,
            "acc_norm": 0.7520661157024794,
            "acc_norm_stderr": 0.03941897526516302
        },
        "harness|hendrycksTest-marketing|0": {
            "acc": 0.8504273504273504,
            "acc_stderr": 0.023365051491753715,
            "acc_norm": 0.8504273504273504,
            "acc_norm_stderr": 0.023365051491753715
        },
        "harness|hendrycksTest-prehistory|0": {
            "acc": 0.6481481481481481,
            "acc_stderr": 0.02657148348071997,
            "acc_norm": 0.6481481481481481,
            "acc_norm_stderr": 0.02657148348071997
        },
        "harness|hendrycksTest-professional_psychology|0": {
            "acc": 0.5490196078431373,
            "acc_stderr": 0.020130388312904528,
            "acc_norm": 0.5490196078431373,
            "acc_norm_stderr": 0.020130388312904528
        },
        "harness|hendrycksTest-high_school_mathematics|0": {
            "acc": 0.3592592592592593,
            "acc_stderr": 0.029252905927251976,
            "acc_norm": 0.3592592592592593,
            "acc_norm_stderr": 0.029252905927251976
        },
        "harness|arc:challenge|0": {
            "acc": 0.41638225255972694,
            "acc_stderr": 0.014405618279436178,
            "acc_norm": 0.43856655290102387,
            "acc_norm_stderr": 0.014500682618212862
        },
        "harness|lambada:openai|0": {
            "ppl": 6.132638767474781,
            "ppl_stderr": 0.2151438143401344,
            "acc": 0.5944110227052203,
            "acc_stderr": 0.006840668923305318
        },
        "harness|hendrycksTest-professional_accounting|0": {
            "acc": 0.4432624113475177,
            "acc_stderr": 0.029634838473766006,
            "acc_norm": 0.4432624113475177,
            "acc_norm_stderr": 0.029634838473766006
        },
        "harness|hendrycksTest-human_aging|0": {
            "acc": 0.6098654708520179,
            "acc_stderr": 0.03273766725459157,
            "acc_norm": 0.6098654708520179,
            "acc_norm_stderr": 0.03273766725459157
        },
        "harness|hendrycksTest-sociology|0": {
            "acc": 0.7860696517412935,
            "acc_stderr": 0.02899690969332891,
            "acc_norm": 0.7860696517412935,
            "acc_norm_stderr": 0.02899690969332891
        },
        "harness|openbookqa|0": {
            "acc": 0.332,
            "acc_stderr": 0.021081766571222856,
            "acc_norm": 0.418,
            "acc_norm_stderr": 0.022080014812228134
        },
        "harness|hendrycksTest-virology|0": {
            "acc": 0.46987951807228917,
            "acc_stderr": 0.03885425420866766,
            "acc_norm": 0.46987951807228917,
            "acc_norm_stderr": 0.03885425420866766
        },
        "harness|arc:easy|0": {
            "acc": 0.6696127946127947,
            "acc_stderr": 0.009651430216428185,
            "acc_norm": 0.6060606060606061,
            "acc_norm_stderr": 0.010026305355981814
        },
        "harness|hendrycksTest-high_school_macroeconomics|0": {
            "acc": 0.5923076923076923,
            "acc_stderr": 0.024915243985987847,
            "acc_norm": 0.5923076923076923,
            "acc_norm_stderr": 0.024915243985987847
        },
        "harness|hendrycksTest-college_chemistry|0": {
            "acc": 0.44,
            "acc_stderr": 0.04988876515698589,
            "acc_norm": 0.44,
            "acc_norm_stderr": 0.04988876515698589
        },
        "harness|hendrycksTest-college_physics|0": {
            "acc": 0.43137254901960786,
            "acc_stderr": 0.04928099597287534,
            "acc_norm": 0.43137254901960786,
            "acc_norm_stderr": 0.04928099597287534
        },
        "harness|hendrycksTest-formal_logic|0": {
            "acc": 0.4444444444444444,
            "acc_stderr": 0.04444444444444449,
            "acc_norm": 0.4444444444444444,
            "acc_norm_stderr": 0.04444444444444449
        },
        "harness|hendrycksTest-electrical_engineering|0": {
            "acc": 0.5655172413793104,
            "acc_stderr": 0.04130740879555498,
            "acc_norm": 0.5655172413793104,
            "acc_norm_stderr": 0.04130740879555498
        },
        "harness|boolq|0": {
            "acc": 0.8418960244648318,
            "acc_stderr": 0.006381064937649044
        },
        "harness|hendrycksTest-high_school_world_history|0": {
            "acc": 0.7552742616033755,
            "acc_stderr": 0.02798569938703643,
            "acc_norm": 0.7552742616033755,
            "acc_norm_stderr": 0.02798569938703643
        },
        "harness|hendrycksTest-high_school_psychology|0": {
            "acc": 0.8091743119266055,
            "acc_stderr": 0.016847676400091077,
            "acc_norm": 0.8091743119266055,
            "acc_norm_stderr": 0.016847676400091077
        },
        "harness|hendrycksTest-global_facts|0": {
            "acc": 0.39,
            "acc_stderr": 0.04902071300001975,
            "acc_norm": 0.39,
            "acc_norm_stderr": 0.04902071300001975
        },
        "harness|piqa|0": {
            "acc": 0.7372143634385201,
            "acc_stderr": 0.010269354068140765,
            "acc_norm": 0.7453754080522307,
            "acc_norm_stderr": 0.010164432237060487
        },
        "harness|hendrycksTest-college_biology|0": {
            "acc": 0.6180555555555556,
            "acc_stderr": 0.040629907841466674,
            "acc_norm": 0.6180555555555556,
            "acc_norm_stderr": 0.040629907841466674
        },
        "harness|hellaswag|0": {
            "acc": 0.5779725154351723,
            "acc_stderr": 0.004928735103635842,
            "acc_norm": 0.7605058753236407,
            "acc_norm_stderr": 0.004259025448541509
        },
        "harness|hendrycksTest-jurisprudence|0": {
            "acc": 0.7407407407407407,
            "acc_stderr": 0.042365112580946315,
            "acc_norm": 0.7407407407407407,
            "acc_norm_stderr": 0.042365112580946315
        },
        "harness|hendrycksTest-public_relations|0": {
            "acc": 0.6181818181818182,
            "acc_stderr": 0.046534298079135075,
            "acc_norm": 0.6181818181818182,
            "acc_norm_stderr": 0.046534298079135075
        },
        "harness|hendrycksTest-high_school_biology|0": {
            "acc": 0.7193548387096774,
            "acc_stderr": 0.0255606047210229,
            "acc_norm": 0.7193548387096774,
            "acc_norm_stderr": 0.0255606047210229
        },
        "harness|hendrycksTest-moral_disputes|0": {
            "acc": 0.6473988439306358,
            "acc_stderr": 0.02572280220089582,
            "acc_norm": 0.6473988439306358,
            "acc_norm_stderr": 0.02572280220089582
        },
        "harness|hendrycksTest-high_school_government_and_politics|0": {
            "acc": 0.772020725388601,
            "acc_stderr": 0.030276909945178263,
            "acc_norm": 0.772020725388601,
            "acc_norm_stderr": 0.030276909945178263
        },
        "harness|hendrycksTest-abstract_algebra|0": {
            "acc": 0.42,
            "acc_stderr": 0.049604496374885836,
            "acc_norm": 0.42,
            "acc_norm_stderr": 0.049604496374885836
        },
        "harness|hendrycksTest-high_school_chemistry|0": {
            "acc": 0.5320197044334976,
            "acc_stderr": 0.03510766597959215,
            "acc_norm": 0.5320197044334976,
            "acc_norm_stderr": 0.03510766597959215
        },
        "harness|hendrycksTest-high_school_microeconomics|0": {
            "acc": 0.6428571428571429,
            "acc_stderr": 0.031124619309328177,
            "acc_norm": 0.6428571428571429,
            "acc_norm_stderr": 0.031124619309328177
        },
        "harness|hendrycksTest-miscellaneous|0": {
            "acc": 0.7535121328224776,
            "acc_stderr": 0.015411308769686936,
            "acc_norm": 0.7535121328224776,
            "acc_norm_stderr": 0.015411308769686936
        },
        "harness|hendrycksTest-philosophy|0": {
            "acc": 0.662379421221865,
            "acc_stderr": 0.026858825879488544,
            "acc_norm": 0.662379421221865,
            "acc_norm_stderr": 0.026858825879488544
        },
        "harness|hendrycksTest-college_medicine|0": {
            "acc": 0.5549132947976878,
            "acc_stderr": 0.03789401760283647,
            "acc_norm": 0.5549132947976878,
            "acc_norm_stderr": 0.03789401760283647
        },
        "harness|hendrycksTest-conceptual_physics|0": {
            "acc": 0.5404255319148936,
            "acc_stderr": 0.03257901482099834,
            "acc_norm": 0.5404255319148936,
            "acc_norm_stderr": 0.03257901482099834
        },
        "harness|winogrande|0": {
            "acc": 0.6400947119179163,
            "acc_stderr": 0.013489609590266807
        },
        "harness|hendrycksTest-high_school_geography|0": {
            "acc": 0.7878787878787878,
            "acc_stderr": 0.029126522834586818,
            "acc_norm": 0.7878787878787878,
            "acc_norm_stderr": 0.029126522834586818
        },
        "harness|hendrycksTest-high_school_physics|0": {
            "acc": 0.3509933774834437,
            "acc_stderr": 0.03896981964257375,
            "acc_norm": 0.3509933774834437,
            "acc_norm_stderr": 0.03896981964257375
        },
        "harness|hendrycksTest-security_studies|0": {
            "acc": 0.7020408163265306,
            "acc_stderr": 0.029279567411065674,
            "acc_norm": 0.7020408163265306,
            "acc_norm_stderr": 0.029279567411065674
        },
        "harness|hendrycksTest-elementary_mathematics|0": {
            "acc": 0.4576719576719577,
            "acc_stderr": 0.02565886886205834,
            "acc_norm": 0.4576719576719577,
            "acc_norm_stderr": 0.02565886886205834
        }
    },
    "task_info": {
        "model": "Qwen/Qwen1.5-7B-Chat-GPTQ-Int4",
        "revision": "main",
        "private": false,
        "params": 2.114,
        "architectures": "Qwen2ForCausalLM",
        "quant_type": "GPTQ",
        "precision": "4bit",
        "weight_dtype": "int4",
        "compute_dtype": "bfloat16",
        "gguf_ftype": "*q4_0.gguf",
        "hardware": "cpu",
        "status": "Waiting",
        "submitted_time": "2024-04-20T05:40:35Z",
        "model_type": "quantization",
        "job_id": -1,
        "job_start_time": null,
        "scripts": "ITREX"
    },
    "quantization_config": {
        "batch_size": 1,
        "bits": 4,
        "block_name_to_quantize": null,
        "cache_block_outputs": true,
        "damp_percent": 0.01,
        "dataset": null,
        "desc_act": false,
        "exllama_config": {
            "version": 1
        },
        "group_size": 128,
        "max_input_length": null,
        "model_seqlen": null,
        "module_name_preceding_first_block": null,
        "modules_in_block_to_quantize": null,
        "pad_token_id": null,
        "quant_method": "gptq",
        "sym": true,
        "tokenizer": null,
        "true_sequential": true,
        "use_cuda_fp16": false,
        "use_exllama": true,
        "disable_exllama": true
    },
    "versions": {
        "harness|hendrycksTest-professional_medicine|0": 1,
        "harness|hendrycksTest-high_school_statistics|0": 1,
        "harness|hendrycksTest-management|0": 1,
        "harness|hendrycksTest-world_religions|0": 1,
        "harness|hendrycksTest-high_school_computer_science|0": 1,
        "harness|hendrycksTest-high_school_us_history|0": 1,
        "harness|hendrycksTest-computer_security|0": 1,
        "harness|hendrycksTest-nutrition|0": 1,
        "harness|hendrycksTest-clinical_knowledge|0": 1,
        "harness|hendrycksTest-business_ethics|0": 1,
        "harness|hendrycksTest-astronomy|0": 1,
        "harness|hendrycksTest-college_mathematics|0": 1,
        "harness|hendrycksTest-econometrics|0": 1,
        "harness|hendrycksTest-high_school_european_history|0": 1,
        "harness|hendrycksTest-anatomy|0": 1,
        "harness|hendrycksTest-medical_genetics|0": 1,
        "harness|hendrycksTest-moral_scenarios|0": 1,
        "harness|hendrycksTest-professional_law|0": 1,
        "harness|hendrycksTest-us_foreign_policy|0": 1,
        "harness|truthfulqa:mc|0": 1,
        "harness|hendrycksTest-machine_learning|0": 1,
        "harness|hendrycksTest-logical_fallacies|0": 1,
        "harness|hendrycksTest-human_sexuality|0": 1,
        "harness|hendrycksTest-college_computer_science|0": 1,
        "harness|hendrycksTest-international_law|0": 1,
        "harness|hendrycksTest-marketing|0": 1,
        "harness|hendrycksTest-prehistory|0": 1,
        "harness|hendrycksTest-professional_psychology|0": 1,
        "harness|hendrycksTest-high_school_mathematics|0": 1,
        "harness|arc:challenge|0": 0,
        "harness|lambada:openai|0": 0,
        "harness|hendrycksTest-professional_accounting|0": 1,
        "harness|hendrycksTest-human_aging|0": 1,
        "harness|hendrycksTest-sociology|0": 1,
        "harness|openbookqa|0": 0,
        "harness|hendrycksTest-virology|0": 1,
        "harness|arc:easy|0": 0,
        "harness|hendrycksTest-high_school_macroeconomics|0": 1,
        "harness|hendrycksTest-college_chemistry|0": 1,
        "harness|hendrycksTest-college_physics|0": 1,
        "harness|hendrycksTest-formal_logic|0": 1,
        "harness|hendrycksTest-electrical_engineering|0": 1,
        "harness|boolq|0": 1,
        "harness|hendrycksTest-high_school_world_history|0": 1,
        "harness|hendrycksTest-high_school_psychology|0": 1,
        "harness|hendrycksTest-global_facts|0": 1,
        "harness|piqa|0": 0,
        "harness|hendrycksTest-college_biology|0": 1,
        "harness|hellaswag|0": 0,
        "harness|hendrycksTest-jurisprudence|0": 1,
        "harness|hendrycksTest-public_relations|0": 1,
        "harness|hendrycksTest-high_school_biology|0": 1,
        "harness|hendrycksTest-moral_disputes|0": 1,
        "harness|hendrycksTest-high_school_government_and_politics|0": 1,
        "harness|hendrycksTest-abstract_algebra|0": 1,
        "harness|hendrycksTest-high_school_chemistry|0": 1,
        "harness|hendrycksTest-high_school_microeconomics|0": 1,
        "harness|hendrycksTest-miscellaneous|0": 1,
        "harness|hendrycksTest-philosophy|0": 1,
        "harness|hendrycksTest-college_medicine|0": 1,
        "harness|hendrycksTest-conceptual_physics|0": 1,
        "harness|winogrande|0": 0,
        "harness|hendrycksTest-high_school_geography|0": 1,
        "harness|hendrycksTest-high_school_physics|0": 1,
        "harness|hendrycksTest-security_studies|0": 1,
        "harness|hendrycksTest-elementary_mathematics|0": 1
    }
}