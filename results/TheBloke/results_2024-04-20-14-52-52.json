{
    "config_general": {
        "lighteval_sha": "1.4",
        "num_few_shot_default": null,
        "num_fewshot_seeds": null,
        "override_batch_size": null,
        "max_samples": null,
        "job_id": -1,
        "start_time": null,
        "end_time": "2024-04-20-14-52-52",
        "total_evaluation_time_secondes": "",
        "model_name": "TheBloke/Llama-2-7B-GPTQ",
        "model_sha": "",
        "model_dtype": "4bit",
        "model_size": 1.131
    },
    "results": {
        "harness|hendrycksTest-business_ethics|0": {
            "acc": 0.42,
            "acc_stderr": 0.049604496374885836,
            "acc_norm": 0.42,
            "acc_norm_stderr": 0.049604496374885836
        },
        "harness|hendrycksTest-college_physics|0": {
            "acc": 0.17647058823529413,
            "acc_stderr": 0.03793281185307809,
            "acc_norm": 0.17647058823529413,
            "acc_norm_stderr": 0.03793281185307809
        },
        "harness|hendrycksTest-virology|0": {
            "acc": 0.4036144578313253,
            "acc_stderr": 0.038194861407583984,
            "acc_norm": 0.4036144578313253,
            "acc_norm_stderr": 0.038194861407583984
        },
        "harness|hendrycksTest-professional_psychology|0": {
            "acc": 0.4068627450980392,
            "acc_stderr": 0.019873802005061173,
            "acc_norm": 0.4068627450980392,
            "acc_norm_stderr": 0.019873802005061173
        },
        "harness|lambada:openai|0": {
            "ppl": 3.6846535771982003,
            "ppl_stderr": 0.07456647774294405,
            "acc": 0.7242383077818746,
            "acc_stderr": 0.006226152973296045
        },
        "harness|hendrycksTest-miscellaneous|0": {
            "acc": 0.48659003831417624,
            "acc_stderr": 0.017873531736510375,
            "acc_norm": 0.48659003831417624,
            "acc_norm_stderr": 0.017873531736510375
        },
        "harness|hendrycksTest-security_studies|0": {
            "acc": 0.4897959183673469,
            "acc_stderr": 0.03200255347893782,
            "acc_norm": 0.4897959183673469,
            "acc_norm_stderr": 0.03200255347893782
        },
        "harness|arc:challenge|0": {
            "acc": 0.4206484641638225,
            "acc_stderr": 0.014426211252508406,
            "acc_norm": 0.4325938566552901,
            "acc_norm_stderr": 0.014478005694182531
        },
        "harness|hendrycksTest-high_school_government_and_politics|0": {
            "acc": 0.5077720207253886,
            "acc_stderr": 0.03608003225569654,
            "acc_norm": 0.5077720207253886,
            "acc_norm_stderr": 0.03608003225569654
        },
        "harness|hendrycksTest-college_medicine|0": {
            "acc": 0.42196531791907516,
            "acc_stderr": 0.037657466938651504,
            "acc_norm": 0.42196531791907516,
            "acc_norm_stderr": 0.037657466938651504
        },
        "harness|hendrycksTest-college_computer_science|0": {
            "acc": 0.27,
            "acc_stderr": 0.044619604333847394,
            "acc_norm": 0.27,
            "acc_norm_stderr": 0.044619604333847394
        },
        "harness|hendrycksTest-econometrics|0": {
            "acc": 0.23684210526315788,
            "acc_stderr": 0.03999423879281336,
            "acc_norm": 0.23684210526315788,
            "acc_norm_stderr": 0.03999423879281336
        },
        "harness|hendrycksTest-high_school_macroeconomics|0": {
            "acc": 0.358974358974359,
            "acc_stderr": 0.024321738484602357,
            "acc_norm": 0.358974358974359,
            "acc_norm_stderr": 0.024321738484602357
        },
        "harness|hendrycksTest-abstract_algebra|0": {
            "acc": 0.27,
            "acc_stderr": 0.044619604333847394,
            "acc_norm": 0.27,
            "acc_norm_stderr": 0.044619604333847394
        },
        "harness|hendrycksTest-moral_disputes|0": {
            "acc": 0.4046242774566474,
            "acc_stderr": 0.026424816594009852,
            "acc_norm": 0.4046242774566474,
            "acc_norm_stderr": 0.026424816594009852
        },
        "harness|hendrycksTest-world_religions|0": {
            "acc": 0.5730994152046783,
            "acc_stderr": 0.03793620616529916,
            "acc_norm": 0.5730994152046783,
            "acc_norm_stderr": 0.03793620616529916
        },
        "harness|hendrycksTest-college_chemistry|0": {
            "acc": 0.28,
            "acc_stderr": 0.04512608598542127,
            "acc_norm": 0.28,
            "acc_norm_stderr": 0.04512608598542127
        },
        "harness|hendrycksTest-conceptual_physics|0": {
            "acc": 0.2978723404255319,
            "acc_stderr": 0.02989614568209546,
            "acc_norm": 0.2978723404255319,
            "acc_norm_stderr": 0.02989614568209546
        },
        "harness|hendrycksTest-jurisprudence|0": {
            "acc": 0.4722222222222222,
            "acc_stderr": 0.04826217294139894,
            "acc_norm": 0.4722222222222222,
            "acc_norm_stderr": 0.04826217294139894
        },
        "harness|piqa|0": {
            "acc": 0.7731229597388466,
            "acc_stderr": 0.009771584259215154,
            "acc_norm": 0.7856365614798694,
            "acc_norm_stderr": 0.009574842136050971
        },
        "harness|hendrycksTest-nutrition|0": {
            "acc": 0.46405228758169936,
            "acc_stderr": 0.02855582751652879,
            "acc_norm": 0.46405228758169936,
            "acc_norm_stderr": 0.02855582751652879
        },
        "harness|hendrycksTest-public_relations|0": {
            "acc": 0.34545454545454546,
            "acc_stderr": 0.04554619617541054,
            "acc_norm": 0.34545454545454546,
            "acc_norm_stderr": 0.04554619617541054
        },
        "harness|hendrycksTest-formal_logic|0": {
            "acc": 0.3333333333333333,
            "acc_stderr": 0.04216370213557835,
            "acc_norm": 0.3333333333333333,
            "acc_norm_stderr": 0.04216370213557835
        },
        "harness|hendrycksTest-college_biology|0": {
            "acc": 0.4375,
            "acc_stderr": 0.04148415739394154,
            "acc_norm": 0.4375,
            "acc_norm_stderr": 0.04148415739394154
        },
        "harness|hendrycksTest-computer_security|0": {
            "acc": 0.46,
            "acc_stderr": 0.05009082659620333,
            "acc_norm": 0.46,
            "acc_norm_stderr": 0.05009082659620333
        },
        "harness|hendrycksTest-machine_learning|0": {
            "acc": 0.4017857142857143,
            "acc_stderr": 0.04653333146973647,
            "acc_norm": 0.4017857142857143,
            "acc_norm_stderr": 0.04653333146973647
        },
        "harness|arc:easy|0": {
            "acc": 0.7487373737373737,
            "acc_stderr": 0.008900141191221648,
            "acc_norm": 0.7209595959595959,
            "acc_norm_stderr": 0.009203588704032645
        },
        "harness|hendrycksTest-logical_fallacies|0": {
            "acc": 0.4601226993865031,
            "acc_stderr": 0.03915857291436972,
            "acc_norm": 0.4601226993865031,
            "acc_norm_stderr": 0.03915857291436972
        },
        "harness|hendrycksTest-global_facts|0": {
            "acc": 0.26,
            "acc_stderr": 0.04408440022768078,
            "acc_norm": 0.26,
            "acc_norm_stderr": 0.04408440022768078
        },
        "harness|hendrycksTest-high_school_mathematics|0": {
            "acc": 0.2777777777777778,
            "acc_stderr": 0.02730914058823017,
            "acc_norm": 0.2777777777777778,
            "acc_norm_stderr": 0.02730914058823017
        },
        "harness|hendrycksTest-human_aging|0": {
            "acc": 0.452914798206278,
            "acc_stderr": 0.03340867501923324,
            "acc_norm": 0.452914798206278,
            "acc_norm_stderr": 0.03340867501923324
        },
        "harness|hendrycksTest-prehistory|0": {
            "acc": 0.4104938271604938,
            "acc_stderr": 0.027371350925124768,
            "acc_norm": 0.4104938271604938,
            "acc_norm_stderr": 0.027371350925124768
        },
        "harness|hendrycksTest-marketing|0": {
            "acc": 0.5555555555555556,
            "acc_stderr": 0.03255326307272487,
            "acc_norm": 0.5555555555555556,
            "acc_norm_stderr": 0.03255326307272487
        },
        "harness|hellaswag|0": {
            "acc": 0.5605457080262896,
            "acc_stderr": 0.004953063404791448,
            "acc_norm": 0.7474606652061342,
            "acc_norm_stderr": 0.004335809614480308
        },
        "harness|hendrycksTest-management|0": {
            "acc": 0.3883495145631068,
            "acc_stderr": 0.04825729337356391,
            "acc_norm": 0.3883495145631068,
            "acc_norm_stderr": 0.04825729337356391
        },
        "harness|hendrycksTest-us_foreign_policy|0": {
            "acc": 0.55,
            "acc_stderr": 0.05,
            "acc_norm": 0.55,
            "acc_norm_stderr": 0.05
        },
        "harness|openbookqa|0": {
            "acc": 0.304,
            "acc_stderr": 0.02059164957122493,
            "acc_norm": 0.42,
            "acc_norm_stderr": 0.022094713229761784
        },
        "harness|hendrycksTest-astronomy|0": {
            "acc": 0.40131578947368424,
            "acc_stderr": 0.03988903703336284,
            "acc_norm": 0.40131578947368424,
            "acc_norm_stderr": 0.03988903703336284
        },
        "harness|hendrycksTest-professional_law|0": {
            "acc": 0.3305084745762712,
            "acc_stderr": 0.012014142101842974,
            "acc_norm": 0.3305084745762712,
            "acc_norm_stderr": 0.012014142101842974
        },
        "harness|hendrycksTest-clinical_knowledge|0": {
            "acc": 0.42641509433962266,
            "acc_stderr": 0.030437794342983045,
            "acc_norm": 0.42641509433962266,
            "acc_norm_stderr": 0.030437794342983045
        },
        "harness|hendrycksTest-human_sexuality|0": {
            "acc": 0.5038167938931297,
            "acc_stderr": 0.04385162325601553,
            "acc_norm": 0.5038167938931297,
            "acc_norm_stderr": 0.04385162325601553
        },
        "harness|hendrycksTest-professional_accounting|0": {
            "acc": 0.3546099290780142,
            "acc_stderr": 0.02853865002887864,
            "acc_norm": 0.3546099290780142,
            "acc_norm_stderr": 0.02853865002887864
        },
        "harness|hendrycksTest-high_school_microeconomics|0": {
            "acc": 0.3487394957983193,
            "acc_stderr": 0.030956636328566548,
            "acc_norm": 0.3487394957983193,
            "acc_norm_stderr": 0.030956636328566548
        },
        "harness|hendrycksTest-professional_medicine|0": {
            "acc": 0.40441176470588236,
            "acc_stderr": 0.029812630701569743,
            "acc_norm": 0.40441176470588236,
            "acc_norm_stderr": 0.029812630701569743
        },
        "harness|hendrycksTest-high_school_european_history|0": {
            "acc": 0.49696969696969695,
            "acc_stderr": 0.03904272341431857,
            "acc_norm": 0.49696969696969695,
            "acc_norm_stderr": 0.03904272341431857
        },
        "harness|hendrycksTest-international_law|0": {
            "acc": 0.49586776859504134,
            "acc_stderr": 0.045641987674327526,
            "acc_norm": 0.49586776859504134,
            "acc_norm_stderr": 0.045641987674327526
        },
        "harness|hendrycksTest-high_school_statistics|0": {
            "acc": 0.2824074074074074,
            "acc_stderr": 0.030701372111510937,
            "acc_norm": 0.2824074074074074,
            "acc_norm_stderr": 0.030701372111510937
        },
        "harness|hendrycksTest-medical_genetics|0": {
            "acc": 0.44,
            "acc_stderr": 0.049888765156985884,
            "acc_norm": 0.44,
            "acc_norm_stderr": 0.049888765156985884
        },
        "harness|boolq|0": {
            "acc": 0.7629969418960245,
            "acc_stderr": 0.007437567381277108
        },
        "harness|hendrycksTest-high_school_us_history|0": {
            "acc": 0.5147058823529411,
            "acc_stderr": 0.035077938347913236,
            "acc_norm": 0.5147058823529411,
            "acc_norm_stderr": 0.035077938347913236
        },
        "harness|hendrycksTest-high_school_chemistry|0": {
            "acc": 0.29064039408866993,
            "acc_stderr": 0.0319474007226554,
            "acc_norm": 0.29064039408866993,
            "acc_norm_stderr": 0.0319474007226554
        },
        "harness|hendrycksTest-high_school_geography|0": {
            "acc": 0.3888888888888889,
            "acc_stderr": 0.0347327959083696,
            "acc_norm": 0.3888888888888889,
            "acc_norm_stderr": 0.0347327959083696
        },
        "harness|hendrycksTest-electrical_engineering|0": {
            "acc": 0.3793103448275862,
            "acc_stderr": 0.04043461861916747,
            "acc_norm": 0.3793103448275862,
            "acc_norm_stderr": 0.04043461861916747
        },
        "harness|hendrycksTest-high_school_world_history|0": {
            "acc": 0.46835443037974683,
            "acc_stderr": 0.03248197400511075,
            "acc_norm": 0.46835443037974683,
            "acc_norm_stderr": 0.03248197400511075
        },
        "harness|hendrycksTest-sociology|0": {
            "acc": 0.6517412935323383,
            "acc_stderr": 0.033687874661154596,
            "acc_norm": 0.6517412935323383,
            "acc_norm_stderr": 0.033687874661154596
        },
        "harness|truthfulqa:mc|0": {
            "mc1": 0.24479804161566707,
            "mc1_stderr": 0.015051869486715013,
            "mc2": 0.3913527963157154,
            "mc2_stderr": 0.013678053717104885
        },
        "harness|hendrycksTest-elementary_mathematics|0": {
            "acc": 0.21693121693121692,
            "acc_stderr": 0.02122708244944505,
            "acc_norm": 0.21693121693121692,
            "acc_norm_stderr": 0.02122708244944505
        },
        "harness|hendrycksTest-high_school_physics|0": {
            "acc": 0.271523178807947,
            "acc_stderr": 0.03631329803969653,
            "acc_norm": 0.271523178807947,
            "acc_norm_stderr": 0.03631329803969653
        },
        "harness|hendrycksTest-high_school_psychology|0": {
            "acc": 0.48073394495412847,
            "acc_stderr": 0.021421402982548878,
            "acc_norm": 0.48073394495412847,
            "acc_norm_stderr": 0.021421402982548878
        },
        "harness|hendrycksTest-high_school_biology|0": {
            "acc": 0.3967741935483871,
            "acc_stderr": 0.027831231605767937,
            "acc_norm": 0.3967741935483871,
            "acc_norm_stderr": 0.027831231605767937
        },
        "harness|winogrande|0": {
            "acc": 0.6795580110497238,
            "acc_stderr": 0.01311508545768171
        },
        "harness|hendrycksTest-moral_scenarios|0": {
            "acc": 0.23798882681564246,
            "acc_stderr": 0.014242630070574915,
            "acc_norm": 0.23798882681564246,
            "acc_norm_stderr": 0.014242630070574915
        },
        "harness|hendrycksTest-philosophy|0": {
            "acc": 0.48231511254019294,
            "acc_stderr": 0.02838032284907713,
            "acc_norm": 0.48231511254019294,
            "acc_norm_stderr": 0.02838032284907713
        },
        "harness|hendrycksTest-high_school_computer_science|0": {
            "acc": 0.37,
            "acc_stderr": 0.04852365870939099,
            "acc_norm": 0.37,
            "acc_norm_stderr": 0.04852365870939099
        },
        "harness|hendrycksTest-college_mathematics|0": {
            "acc": 0.3,
            "acc_stderr": 0.046056618647183814,
            "acc_norm": 0.3,
            "acc_norm_stderr": 0.046056618647183814
        },
        "harness|hendrycksTest-anatomy|0": {
            "acc": 0.4074074074074074,
            "acc_stderr": 0.042446332383532286,
            "acc_norm": 0.4074074074074074,
            "acc_norm_stderr": 0.042446332383532286
        }
    },
    "task_info": {
        "model": "TheBloke/Llama-2-7B-GPTQ",
        "revision": "main",
        "private": false,
        "params": 1.131,
        "architectures": "LlamaForCausalLM",
        "quant_type": "GPTQ",
        "precision": "4bit",
        "weight_dtype": "int4",
        "compute_dtype": "bfloat16",
        "gguf_ftype": "*q4_0.gguf",
        "hardware": "cpu",
        "status": "Running",
        "submitted_time": "2024-04-19T13:11:57Z",
        "model_type": "quantization",
        "job_id": -1,
        "job_start_time": null,
        "scripts": "ITREX"
    },
    "quantization_config": {
        "bits": 4,
        "group_size": 128,
        "damp_percent": 0.01,
        "desc_act": false,
        "sym": true,
        "true_sequential": true,
        "model_name_or_path": null,
        "model_file_base_name": "model",
        "quant_method": "gptq",
        "disable_exllama": true
    },
    "versions": {
        "harness|hendrycksTest-business_ethics|0": 1,
        "harness|hendrycksTest-college_physics|0": 1,
        "harness|hendrycksTest-virology|0": 1,
        "harness|hendrycksTest-professional_psychology|0": 1,
        "harness|lambada:openai|0": 0,
        "harness|hendrycksTest-miscellaneous|0": 1,
        "harness|hendrycksTest-security_studies|0": 1,
        "harness|arc:challenge|0": 0,
        "harness|hendrycksTest-high_school_government_and_politics|0": 1,
        "harness|hendrycksTest-college_medicine|0": 1,
        "harness|hendrycksTest-college_computer_science|0": 1,
        "harness|hendrycksTest-econometrics|0": 1,
        "harness|hendrycksTest-high_school_macroeconomics|0": 1,
        "harness|hendrycksTest-abstract_algebra|0": 1,
        "harness|hendrycksTest-moral_disputes|0": 1,
        "harness|hendrycksTest-world_religions|0": 1,
        "harness|hendrycksTest-college_chemistry|0": 1,
        "harness|hendrycksTest-conceptual_physics|0": 1,
        "harness|hendrycksTest-jurisprudence|0": 1,
        "harness|piqa|0": 0,
        "harness|hendrycksTest-nutrition|0": 1,
        "harness|hendrycksTest-public_relations|0": 1,
        "harness|hendrycksTest-formal_logic|0": 1,
        "harness|hendrycksTest-college_biology|0": 1,
        "harness|hendrycksTest-computer_security|0": 1,
        "harness|hendrycksTest-machine_learning|0": 1,
        "harness|arc:easy|0": 0,
        "harness|hendrycksTest-logical_fallacies|0": 1,
        "harness|hendrycksTest-global_facts|0": 1,
        "harness|hendrycksTest-high_school_mathematics|0": 1,
        "harness|hendrycksTest-human_aging|0": 1,
        "harness|hendrycksTest-prehistory|0": 1,
        "harness|hendrycksTest-marketing|0": 1,
        "harness|hellaswag|0": 0,
        "harness|hendrycksTest-management|0": 1,
        "harness|hendrycksTest-us_foreign_policy|0": 1,
        "harness|openbookqa|0": 0,
        "harness|hendrycksTest-astronomy|0": 1,
        "harness|hendrycksTest-professional_law|0": 1,
        "harness|hendrycksTest-clinical_knowledge|0": 1,
        "harness|hendrycksTest-human_sexuality|0": 1,
        "harness|hendrycksTest-professional_accounting|0": 1,
        "harness|hendrycksTest-high_school_microeconomics|0": 1,
        "harness|hendrycksTest-professional_medicine|0": 1,
        "harness|hendrycksTest-high_school_european_history|0": 1,
        "harness|hendrycksTest-international_law|0": 1,
        "harness|hendrycksTest-high_school_statistics|0": 1,
        "harness|hendrycksTest-medical_genetics|0": 1,
        "harness|boolq|0": 1,
        "harness|hendrycksTest-high_school_us_history|0": 1,
        "harness|hendrycksTest-high_school_chemistry|0": 1,
        "harness|hendrycksTest-high_school_geography|0": 1,
        "harness|hendrycksTest-electrical_engineering|0": 1,
        "harness|hendrycksTest-high_school_world_history|0": 1,
        "harness|hendrycksTest-sociology|0": 1,
        "harness|truthfulqa:mc|0": 1,
        "harness|hendrycksTest-elementary_mathematics|0": 1,
        "harness|hendrycksTest-high_school_physics|0": 1,
        "harness|hendrycksTest-high_school_psychology|0": 1,
        "harness|hendrycksTest-high_school_biology|0": 1,
        "harness|winogrande|0": 0,
        "harness|hendrycksTest-moral_scenarios|0": 1,
        "harness|hendrycksTest-philosophy|0": 1,
        "harness|hendrycksTest-high_school_computer_science|0": 1,
        "harness|hendrycksTest-college_mathematics|0": 1,
        "harness|hendrycksTest-anatomy|0": 1
    }
}